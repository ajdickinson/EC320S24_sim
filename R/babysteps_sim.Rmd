---
title: "Simulation Exercise"
author: "YOUR NAME HERE"
date: "`r Sys.Date()`"
output: html_document
---

# Introduction

This document is an RMarkdown (.Rmd) file. RMarkdown is a versatile tool that allows you to create documents integrating code, results, and narrative text. It supports reproducibility by embedding code directly in the document and can output to various formats, including HTML, PDF, and Word. The remaining problem sets and the take-home final will all be using RMarkdown.

## Structure of an RMarkdown Document

An RMarkdown document typically consists of:

1. **YAML Header:** Contains metadata like title, author, and output format.
  - The YAML header will be provided for you.
  
2. **Narrative Text:** Written in Markdown, includes headings, lists, and formatted text
  - Markdown is a lightweight markup language with plain text formatting syntax
  - You'll use Markdown to write your answers and explanations
  - Basic Markdown syntax includes:
    - `#` for headings (e.g., `# Heading 1`, `## Heading 2`)
    - `*` for bullet points (e.g., `* Item 1`, `* Item 2`)
    - `1.` for numbered lists (e.g., `1. Item 1`, `2. Item 2`)
    - `**` for bold text (e.g., `**bold text**`)
    - `*` for italic text (e.g., `*italic text*`)
    - `[link text](url)` for hyperlinks (e.g., `[Google](https://www.google.com)`)
    - `![alt text](url)` for images (e.g., `![R logo](https://www.r-project.org/logo/Rlogo.png)`)
  
3. **Code Chunks:** Embedded R code blocks for executing and displaying results
  - Code chunks are enclosed by three backticks and `{r}`
  - You can run R code in these chunks and display the output
  - You can also include plots and tables generated by R code
  - All coding exercises will be done in code chunks
  - Example:

```{r}
# Generate simple vector
x = 1:10
x
```

4. **Output:** The final document can be rendered in various formats
  - In this class, we will always compile to HTML

# Babystep Simulation

## Objective

The objective of this exercise is to simulate a dataset with two random variables 
that have a known linear relationship and estimate the coefficients of that relationship 
using Ordinary Least Squares (OLS) regression. We will generate random numbers from 
defined distributions and fit a linear model to estimate the coefficients.

## Setup

First, we load the necessary packages and set the seed for reproducibility.

```{r setup, message=FALSE, warning=FALSE}
# Load packages
# library(tidyverse)

# Set seed for reproducibility

```

## Simulate Dataframe with Two Random Variables with a Known Relationship

We will define the sample size, generate the predictor variable `x` and the error term `u`, then calculate the dependent variable `y`.

```{r simulate-data}
# Define the sample size

# Generate the data
# Generate the predictor variable `x` from a normal distribution
# Generate the error term `u` from a normal distribution
# Generate the dependent variable `y` as a linear function of `x` and `u`

# Create a dataframe `sim_df` with `x` and `y`

```

Now, before ever running a regression, it is a good idea to plot:

```{r plot-data}
# Plot the relationship between `x` and `y` with ggplot2
```

## Estimate the Relationship between `x` and `y`

Now, we fit an OLS model to `sim_df` to estimate the relationship between `y` and `x` and find the coefficients $\beta_1$ and $\beta_2$.

```{r estimate-relationship}
# Fit an OLS model to `sim_df` to estimate the relationship between `y` and `x`

# Extract the coefficients

beta_0 = 0; beta_1 = 1

```

__Note__: One trick I really like to use is in-line `R` codeblocks to display the estimated coefficients. This can be done with the following syntax, `r beta_0` ("r beta_0", just replace the quotes with backticks). This is a great way to display the estimated coefficients to answer questions in the narrative text.

For example, the estimated coefficients are $\widehat{\beta}_0 = `r beta_0`$ and $\widehat{\beta}_1 = `r beta_1`$.


# Monte Carlo Study

Okay, now that we have laid the groundwork, let's conduct a simulation study to estimate the coefficients of the linear relationship between `x` and `y`. We will repeat the process multiple times, each time generating a new dataset, fitting a model, and returning the estimated coefficients.

We will do so using something called a for loop. A for loop is a programming construct that allows you to repeat a block of code multiple times. Like I've said in class before, computers are really good at doing the same thing over and over again. Now that we have a babystep simulation framework, we can package it up into a __function__ and run it as much as we want.


## Functions

Functions are a way to encapsulate a block of code that performs a specific task. They take input arguments, perform some operations, and return an output. Functions are useful for modularizing code, making it more readable, and reusing it across different parts of your script.

Here, we define a function `mcstudy` that performs the babystep simulation. Each time, this function generates a new dataset, fits a model, and returns the estimated coefficients.

```{r simulation-function, eval = FALSE}
mcstudy = function(i, n, beta1, sd_x, sd_u) {

  # Generate data
  x = rnorm(n, mean = beta1, sd = sd_x)
  u = rnorm(n, mean = 0, sd = sd_u)
  y = 1 + beta1 * x + u 

  # Create dataframe object
  sim_df = data.frame(x, y) %>%
    arrange(x)

  # Estimate using OLS
  model = lm(y ~ x, data = sim_df)
  b1 = coef(model)["(Intercept)"]
  b2 = coef(model)["x"]

  # Another estimator
  b2_naive = sim_df %>% 
    summarise(
      b2_naive = (last(y) - first(y)) / 
                 (last(x) - first(x))
    ) %>% 
    pull(b2_naive)

  output_df = data.frame(
    i = i,
    b1 = b1,
    b2 = b2,
    b2_naive = b2_naive
  )

  return(output_df)
}
```

Now that we have defined the function, we can run a single simulation using the function `mcstudy` and display the results.

```{r single-simulation, eval = FALSE}
# Run a single simulation
single_sim = mcstudy(i = 1, n = 100, beta1 = 3, sd_x = 2, sd_u = 1)
single_sim
```


## Run the Monte Carlo Study (a lot)

Now, we will run the Monte Carlo study multiple times to estimate the coefficients of the linear relationship between `x` and `y`. We will store the results in a dataframe `mc_results`.

```{r monte-carlo, eval=FALSE}
# Set the number of simulations
n_sims = 1000
beta1 = 3
sd_x = 2
sd_u = 1

# Run the Monte Carlo study with an `lapply`
mc_results = lapply(
    X = 1:n_sims, FUN = mcstudy, 
    n = n_sims, beta1 = beta1, sd_x = sd_x, sd_u = sd_u
  ) %>%
  bind_rows()
```

Now, let's plot the results of the Monte Carlo study to see how well our OLS estimates perform.

```{r plot-monte-carlo, eval=FALSE}
# Plot the distribution of estimated coefficients using ggplot2
p = ggplot(mc_results, aes(x = b2)) +
  geom_histogram(binwidth = 0.01, fill = "grey79", color = "white") +
  geom_vline(xintercept = beta1, linetype = "dashed", color = "black") +
  labs(
    title = "Distribution of Estimated Coefficients",
    x = "Estimated Coefficient",
    y = "Frequency"
  ) + 
  theme_minimal()

p
```

Now if we add the naive estimates, we can see how they compare:

```{r plot-monte-carlo-naive, eval=FALSE}
# Plot the distribution of estimated coefficients using ggplot2
p + geom_histogram(
    aes(x = b2_naive),
    binwidth = 0.01, fill = "grey25", alpha = 0.7, color = "white"
    ) +
  labs(
    title = "Comparing two unbiased estimators",
    x = "Estimated Coefficient",
    y = "Frequency"
  )
```


